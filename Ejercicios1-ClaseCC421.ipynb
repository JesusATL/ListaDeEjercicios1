{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curso de Inteligencia Artificial\n",
    "\n",
    "### Lista de ejercicios de clase CC421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 1\n",
    "\n",
    "Realiza el experimento donde se lanzan monedas como lo indica el siguiente código:\n",
    "\n",
    "```\n",
    "experimento = np.random.randint(0,2, size=10)\n",
    "```\n",
    "Pero esta vez muchas veces, digamos 10,000 veces. Crea una simulación llamada `coin_matrix` para averiguar la distribución del número de caras al lanzar 10 monedas a la vez, usando la función `randint`, con los mismos argumentos, 0 y 2, pero esta vez que el tamaño sea una array bidimensional. \n",
    "\n",
    "Muestra tus resultados con un array más pequeño.\n",
    "\n",
    "Selecciona los los primeros 25 elementos del array, que contienen el número de caras en cada experimento y utiliza algunos métodos útiles para realizar estadísticas, como media, mediana, mínima y máxima, y ​​desviación estándar.\n",
    "\n",
    "¿Cuál es  la distribución del número de caras que obtenemos en el experimento?\n",
    "\n",
    "¿Cuántas caras obtuvimos en cada experimento?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 2\n",
    "\n",
    "Las sumas parciales son un equivalente aproximado de una integral. De hecho, el cálculo define una integral como una suma infinita de elementos infinitesimales. Las diferencias parciales $arr_{i + 1} - arr_ i$ son un equivalente aproximado de una derivada. `numpy` no proporciona una herramienta para calcular diferencias parciales de un array. Escribe un programa que, dado un array , calcule las diferencias parciales de los elementos de ese array. Debes suponer que el array es numérico. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 3\n",
    "\n",
    "El siguiente es un programa (con problemas molestos) pero funcional. Corrige el código de manera que el programa utilice los datos `Canadian lynx trappings` e informe el total `lynx trappings`  por década (diez años), clasificadas en orden inverso (la década más `productiva` primero).El programa debe descargar el archivo de datos en el directorio de la `caché`, pero solo si el archivo aún no está en la caché. Si el directorio no existe, lo  creará. \n",
    "\n",
    "El programa debería guardar los resultados como un archivo CSV en el directorio `doc`. Si el directorio no existe, lo debe crear. \n",
    "Se utiliza la dirección : https://github.com/vincentarelbundock/Rdatasets/blob/master/csv/datasets/lynx.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "# Agunas constantes\n",
    "SRC_HOST = \"https://vincentarelbundock.github.io\"\n",
    "ARCHIVO = \"/lynx.csv\"\n",
    "SRC_NOMBRE = SRC_HOST + \"/Rdatasets/csv/datasets\" + ARCHIVO\n",
    "CACHE = \"cache\"\n",
    "DOC = \"doc\"\n",
    "\n",
    "# Preparamos los directorios si es necesario \n",
    "if not os.path.isdir(CACHE):\n",
    "    os.mkdir(CACHE)\n",
    "if not os.path.isdir(DOC):\n",
    "    os.mkdir(DOC)\n",
    "\n",
    "# Compruebe si el archivo esta en caché, se guardar en caché si no esta\n",
    "if not os.path.isfile(CACHE + ARCHIVO):\n",
    "    try:\n",
    "        src = urllib.request.urlopen(SRC_NOMBRE)\n",
    "        lynx = pd.read_csv(src, index_col=0 )\n",
    "    except:\n",
    "        print(\"No se puede acceder %f.\" % SRC_NOMBRE)\n",
    "        quit()\n",
    "  # Crea un  data frame\n",
    "    lynx.to_csv(CACHE + ARCHIVO, index=False)\n",
    "else:\n",
    "    lynx = pd.read_csv(CACHE + ARCHIVO, index_col=0)\n",
    "\n",
    "# Agregamos la columna \"decade\"\n",
    "#lynx[\"decada\"] = (lynx['time'] / 10).round() * 10\n",
    "\n",
    "# Agregar y ordenar \n",
    "#por_decada = lynx.groupby(\"decada\").sum()\n",
    "#por_decada = por_decada.sort_values(by=\"lynx\", ascending=False)\n",
    "\n",
    "# Guardamos los resultados\n",
    "#por_decada[\"lynx\"].to_csv(DOC + ARCHIVO)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta corregida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 4\n",
    "\n",
    "Escribe un programa que muestre o guarde como archivo PDF un gráfico circular de los estados de EE. UU. Agrupados por letra primera inicial. Necesitarás una lista de nombres de estados o abreviaturas para trabajar en este problema. Puedes obtener la data desde el sitio web: https://worldpopulationreview.com/states/state-abbreviations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 5\n",
    "\n",
    "Escribe un programa que agrupe sitios masivos de redes sociales en línea (MOSN) por el número de usuarios registrados y el ranking global de páginas de Alexa. Analiza facebook.\n",
    "\n",
    "Debido a que los rangos y tamaños de los sitios varían en un amplio rango, usa una escala logarítmica tanto para el clustering como para la presentación. \n",
    " Utiliza el conjunto de datos `mosn.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 6\n",
    "\n",
    "El director de recursos humanos te pide que respondas algunas preguntas descriptivas sobre los empleados, utilizando este conjunto de datos `Employees.xlsx` para responderlas:\n",
    "\n",
    "\n",
    "- ¿Cuántos empleados hay por departamento en el conjunto de datos?\n",
    "- ¿Cuál es la tasa de deserción general?\n",
    "- ¿Cuál es la tarifa promedio por hora?\n",
    "- ¿Cuál es el promedio de años en la empresa?\n",
    "- ¿Quiénes son los 5 empleados con más años en la empresa?\n",
    "- ¿Qué tan satisfechos están los empleados en general? \n",
    "Transforma los datos en una categoría ordinal.\n",
    "\n",
    "Presenta gráficos de visualización.\n",
    "\n",
    "Después de echar un vistazo a tus respuestas, el director de recursos humanos le hace más preguntas:\n",
    "\n",
    "- Dame la lista de los empleados con bajo nivel de satisfacción laboral (usa enmascaramiento o indexación booleana).\n",
    "  \n",
    "- Dame la lista de los empleados con bajo nivel de satisfacción laboral y calificación de rendimiento.\n",
    "\n",
    "- Compara los empleados con satisfacción laboral baja y muy alta en las siguientes variables: edad, departamento, distancia desde el hogar, tarifa por hora, ingreso mensual y años en la empresa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tus soluciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pregunta 7\n",
    "\n",
    "Sea el siguiente código. Explica que lo que hace y sus resultados.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "\n",
    "def predict(X, w, b):\n",
    "    return X * w + b\n",
    "def loss(X, Y, w, b):\n",
    "    return np.average((predict(X, w, b) - Y) ** 2)\n",
    "\n",
    "def gradient(X, Y, w, b):\n",
    "    w_gradient = 2 * np.average(X * (predict(X, w, b) - Y))\n",
    "    b_gradient = 2 * np.average(predict(X, w, b) - Y)\n",
    "    return (w_gradient, b_gradient)\n",
    "def train(X, Y, iterations, lr):\n",
    "    w = b = 0\n",
    "    for i in range(iterations):\n",
    "        print(\"Iteracion %4d => Perdida: %.10f\" % (i, loss(X, Y, w, b)))\n",
    "        w_gradient, b_gradient = gradient(X, Y, w, b)\n",
    "        w -= w_gradient * lr\n",
    "        b -= b_gradient * lr\n",
    "    return w, b\n",
    "\n",
    "\n",
    "X, Y = np.loadtxt(\"pizza.txt\", skiprows=1, unpack=True)\n",
    "w, b = train(X, Y, iterations=20000, lr=0.001)\n",
    "print(\"\\nw=%.10f, b=%.10f\" % (w, b))\n",
    "print(\"Prediccion: x=%d => y=%.2f\" % (20, predict(20, w, b)))\n",
    "```\n",
    "\n",
    "En el código anterior, se utiliza una **tasa de aprendizaje** de 0,001. Intenta aumentar la tasa de aprendizaje y es posible que observes que en algún momento la pérdida comienza a aumentar en lugar de disminuir. ¿Te imaginas por qué?. ¿Qué sucede con una tasa de aprendizaje muy grande? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 8\n",
    "\n",
    "Sea el archivo de código dado. Aumenta el código y carga los datos `life-expectancy-without-country-names` y entrena el modelo para 5 millones de iteraciones con una pequeña tasa de aprendizaje de 0,0001.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "\n",
    "def predict(X, w):\n",
    "    return np.matmul(X, w)\n",
    "def loss(X, Y, w):\n",
    "    return np.average((predict(X, w) - Y) ** 2)\n",
    "\n",
    "def gradient(X, Y, w):\n",
    "    return 2 * np.matmul(X.T, (predict(X, w) - Y)) / X.shape[0]\n",
    "\n",
    "def train(X, Y, iterations, lr):\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    for i in range(iterations):\n",
    "        print(\"Iteracion %4d => Perdida: %.20f\" % (i, loss(X, Y, w)))\n",
    "        w -= gradient(X, Y, w) * lr\n",
    "    return w\n",
    "```\n",
    "\n",
    "\n",
    "El entrenamiento tomó unos minutos en mi computadora. Al final, las predicciones sobre los primeros 10 países se ven así:\n",
    "\n",
    "```\n",
    "X[0] -> 75.5160 (etiqueta: 76)\n",
    "X[1] -> 75.2524 (etiqueta: 74)\n",
    "X[2] -> 77.4308 (etiqueta: 82)\n",
    "X[3] -> 77.2738 (etiqueta: 81)\n",
    "X[4] -> 70.1363 (etiqueta: 71)\n",
    "X[5] -> 76.0045 (etiqueta: 75)\n",
    "X[6] -> 73.4622 (etiqueta: 76)\n",
    "X[7] -> 66.2005 (etiqueta: 71)\n",
    "X[8] -> 76.3621 (etiqueta: 75)\n",
    "X[9] -> 75.6892 (etiqueta: 72)\n",
    "```\n",
    "\n",
    "\n",
    "Algunas de las predicciones están bastante cerca de las etiquetas. Otros fallan bastante mal en el objetivo. La razón de esos fallos es que el modelo basado en hiperplano es demasiado simple para aproximarse a los ejemplos. Cuando introdujimos la regresión lineal, dijimos que puede aproximar los puntos con una línea solo si los puntos están aproximadamente alineados para empezar. Si agrega más dimensiones a los puntos, se aplica el mismo razonamiento. Parece que para aproximarnos a este complejo conjunto de datos del mundo real, necesitamos una forma *no lineal*, en lugar de un hiperplano *lineal*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 9\n",
    "\n",
    "Sobre los datos iris: ¿puedes predecir la especie de un iris usando medidas de pétalos y sépalos?\n",
    "\n",
    "- Lee los datos del iris en un Pandas DataFrame, incluidos los nombres de las columnas.\n",
    " \n",
    "- Reúna información básica sobre los datos.\n",
    "\n",
    "- Utiliza sorting, split-apply-combine y/o visualización para buscar diferencias entre especies.\n",
    "\n",
    "- Escribe un conjunto de reglas que podrían usarse para predecir especies en función de las mediciones del iris.\n",
    "\n",
    "- Define una función que acepte una fila de datos y devuelva una especie predicha. Luego, usa esa función para hacer predicciones para todas las filas de datos existentes y verifique la precisión de sus predicciones. \n",
    "\n",
    "Utiliza el siguiente url: http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tus respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 10\n",
    "\n",
    "Descripción de los datos:\n",
    "\n",
    "- `yelp.json` es el formato original del archivo `yelp.csv` que contiene los mismos datos, en un formato más conveniente. Ambos archivos están en este repositorio, por lo que no es necesario descargar los datos del sitio web de Kaggle.\n",
    "\n",
    "- Cada observación en este conjunto de datos es una revisión de un negocio hecho por un usuario en particular.\n",
    "\n",
    "- La columna `stars` es el número de estrellas (de 1 a 5) que el revisor asigna a la empresa. (Las estrellas más altas son mejores). En otras palabras, es la calificación de la empresa por parte de la persona que escribió la reseña.\n",
    "\n",
    "- La columna `cool` es el número de votos interesantes que recibió esta reseña de otros usuarios de Yelp. Todas las reseñas comienzan con 0 votos `cool` y no hay límite para la cantidad de votos  que puedan recibir una reseña. En otras palabras, es una calificación de la revisión en sí misma, no una calificación de la empresa.\n",
    "\n",
    "- Las columnas `useful` y `fun` son similares a la columna `cool`.\n",
    "\n",
    "Preguntas:\n",
    "\n",
    " - Lee `yelp.csv` en un DataFrame. Ignore el archivo `yelp.csv` y construye este DataFrame tu mismo desde `yelp.json`. Esto implica leer los datos en Python, decodificar el JSON, convertirlo en un DataFrame y agregar columnas individuales para cada uno de los tipos de votos.\n",
    " \n",
    "- Explora la relación entre cada uno de los tipos de votos `(cool/useful/funny)` y la cantidad de estrellas.\n",
    "\n",
    "- Defina las características  `(cool/useful/funny)` y las estrellas como respuesta.\n",
    "\n",
    "- Ajusta un modelo de regresión lineal e interpreta los coeficientes. ¿Los coeficientes tienen sentido intuitivo? Explora el sitio web de Yelp para ver si detecta tendencias similares.\n",
    "\n",
    "- Evalúa el modelo dividiéndolo en conjuntos de entrenamiento y prueba y calculando el RMSE. ¿Tiene el RMSE un sentido intuitivo?\n",
    "\n",
    "- Intenta eliminar algunas de las características y observa si el RMSE mejora. Piensa en algunas características nuevas que podrías crear a partir de los datos existentes que podrían predecir la respuesta.  Descubre cómo crear esas características en Pandas, agréguelas a tu modelo y vea si el RMSE mejora.\n",
    "\n",
    "- Compara tu mejor RMSE en el conjunto de pruebas con el RMSE para el *modelo nulo*, que es el modelo que ignora todas las características y simplemente predice el valor de respuesta promedio en el conjunto de pruebas.\n",
    "\n",
    "- En lugar de tratar esto como un problema de regresión, trátalo como un problema de clasificación y vea qué exactitud de prueba puede lograr con KNN.\n",
    "\n",
    "- Descubra cómo usar la regresión lineal para la clasificación y compara su exactitud de clasificación con la exactitud de KNN. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tus respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
